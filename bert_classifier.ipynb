{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from core.data_utils import SCget_data, SCloadCorpus\n",
    "from core.models import model_selector\n",
    "from core.data_utils.build_graph import get_data\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import TensorDataset\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "_basepath = \"./data/sentiment/\"\n",
    "_model = \"bert\"\n",
    "_bert = \"roberta-base\"\n",
    "_max_length = 128\n",
    "_batch_size = 32\n",
    "_lr = 1e-4\n",
    "_epochs=10\n",
    "_device=\"cuda:1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1707 1707 488 488\n"
     ]
    }
   ],
   "source": [
    "x_tr, y_tr, x_ts, y_ts, x_val, y_val, x_all, y_all, adj = SCget_data(_basepath)\n",
    "text = SCloadCorpus(_basepath)\n",
    "\n",
    "y_tr_np = y_tr.toarray()\n",
    "y_ts_np = y_ts.toarray()\n",
    "\n",
    "y_tr_orig = y_tr_np[:int(len(y_tr_np)/2), :]\n",
    "y_tr_new = y_tr_np[int(len(y_tr_np)/2):, :]\n",
    "y_ts_orig = y_ts_np[:int(len(y_ts_np)/2), :]\n",
    "y_ts_new = y_ts_np[int(len(y_ts_np)/2):, :]\n",
    "\n",
    "train_text = text[:x_tr.shape[0]]\n",
    "val_text = text[x_tr.shape[0]:x_tr.shape[0] + x_val.shape[0]]\n",
    "test_text = text[x_tr.shape[0] + x_val.shape[0]:]\n",
    "\n",
    "train_text_orig = train_text[:int(len(y_tr_np)/2)]\n",
    "train_text_new  = train_text[int(len(y_tr_np)/2):]\n",
    "test_text_orig  = test_text[:int(len(y_ts_np)/2)]\n",
    "test_text_new   = test_text[int(len(y_ts_np)/2):]\n",
    "\n",
    "print(len(train_text_orig), len(train_text_new), len(test_text_orig), len(test_text_new))\n",
    "\n",
    "train_loader = DataLoader((train_text_orig, y_tr_orig), batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader((test_text_new, y_ts_new), batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = model_selector(_model)(_bert, 2, 0.5)\n",
    "model.to(_device)\n",
    "\n",
    "optim = torch.optim.Adam(model.classifier.parameters(), lr=_lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "_input = model.tokenizer(text, max_length=_max_length,\n",
    "                                    truncation=True, padding='max_length',\n",
    "                                    return_tensors='pt')\n",
    "input_ids, attention_mask = _input.input_ids, _input.attention_mask\n",
    "\n",
    "train_orig_iid, train_orig_attm = input_ids[:int(len(y_tr_np)/2)], attention_mask[:int(len(y_tr_np)/2)]\n",
    "train_new_iid, train_new_attm = input_ids[int(len(y_tr_np)/2):], attention_mask[int(len(y_tr_np)/2):]\n",
    "test_orig_iid, test_orig_attm = input_ids[y_tr.shape[0]+y_val.shape[0]:int(len(y_ts_np)/2)], attention_mask[y_tr.shape[0]+y_val.shape[0]:int(len(y_ts_np)/2)]\n",
    "test_new_iid, test_new_attm = input_ids[y_tr.shape[0]+y_val.shape[0]+int(len(y_ts_np)/2):], attention_mask[y_tr.shape[0]+y_val.shape[0]+int(len(y_ts_np)/2):]\n",
    "\n",
    "train_data = TensorDataset(train_orig_iid, train_orig_attm, torch.tensor(y_tr_orig))\n",
    "test_data = TensorDataset(test_new_iid, test_new_attm, torch.tensor(y_ts_new))\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=len(test_data), shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train acc: 0.36363636363636365, Test acc: 0.5389344262295082\n",
      "Train acc: 0.6363636363636364, Test acc: 0.5614754098360656\n",
      "Train acc: 0.45454545454545453, Test acc: 0.5225409836065574\n",
      "Train acc: 0.5454545454545454, Test acc: 0.5922131147540983\n",
      "Train acc: 0.6363636363636364, Test acc: 0.610655737704918\n",
      "Train acc: 0.7272727272727273, Test acc: 0.6004098360655737\n",
      "Train acc: 0.5454545454545454, Test acc: 0.6454918032786885\n",
      "Train acc: 0.45454545454545453, Test acc: 0.5635245901639344\n",
      "Train acc: 0.8181818181818182, Test acc: 0.6290983606557377\n",
      "Train acc: 0.6363636363636364, Test acc: 0.6372950819672131\n"
     ]
    }
   ],
   "source": [
    "for e in range(_epochs):\n",
    "    train_acc_mean = 0.0\n",
    "    for i, (ids, attm, y) in enumerate(train_loader):\n",
    "        model.train()\n",
    "        ids = ids.to(_device)\n",
    "        attm = attm.to(_device)\n",
    "        y = y.to(_device)\n",
    "        optim.zero_grad()\n",
    "        pred = model(ids, attm)\n",
    "        loss = F.nll_loss(pred, torch.argmax(y, axis=1))\n",
    "        loss.backward()\n",
    "        optim.step()\n",
    "        train_loss = loss.item()\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "            y_true = y.argmax(axis=1).detach().cpu()\n",
    "            y_pred = pred.argmax(axis=1).detach().cpu()\n",
    "            train_acc = accuracy_score(y_true, y_pred)\n",
    "            train_acc_mean += train_acc\n",
    "        \n",
    "    train_acc_mean /= len(train_loader)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, (ids, attm, y) in enumerate(test_loader):\n",
    "            ids = ids.to(_device)\n",
    "            attm = attm.to(_device)\n",
    "            pred = model(ids, attm)\n",
    "            y_true = y.argmax(axis=1).detach().cpu()\n",
    "            y_pred = pred.argmax(axis=1).detach().cpu()\n",
    "            test_acc = accuracy_score(y_pred, y_true)\n",
    "\n",
    "    print(f\"Train acc: {train_acc}, Test acc: {test_acc}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
